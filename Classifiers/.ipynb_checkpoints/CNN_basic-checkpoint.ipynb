{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grappling with a CNN\n",
    "\n",
    "This notebook will convince me that I can build a CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorboard\n",
    "import psycopg2\n",
    "from PIL import Image\n",
    "import os\n",
    "import h5py\n",
    "import datetime\n",
    "%load_ext tensorboard\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are stored locally in a PostgreSQL database. So let's connect and get them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to connect to PostgreSQL.\n",
      "Selecting rows from origami_images table.\n",
      "PostgreSQL connection closed.\n"
     ]
    }
   ],
   "source": [
    "# Read in data from origami database in PostgreSQL\n",
    "try:\n",
    "    print('Attempting to connect to PostgreSQL.')\n",
    "    conn = None\n",
    "    conn = psycopg2.connect(host='localhost', database='origami', user='postgres', password='postgres')\n",
    "    cur = conn.cursor()\n",
    "    # SELECT image classifications and file paths from PostgreSQL\n",
    "    sql_select = \"SELECT image_class, image_path FROM origami_images;\"\n",
    "    cur.execute(sql_select)\n",
    "    print('Selecting rows from origami_images table.')\n",
    "    origami_images = cur.fetchall()\n",
    "except (Exception, psycopg2.DatabaseError) as error:\n",
    "    print(error)\n",
    "finally:\n",
    "    if conn is not None:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        print('PostgreSQL connection closed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us `origami_images`, a list of tuples, each of which gives a class and a file path for the associated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166\n",
      "('butterfly', '/Users/wwatson/Desktop/Insight/Project/origami/Images/downloads/butterfly/26.jpeg')\n"
     ]
    }
   ],
   "source": [
    "print(len(origami_images))\n",
    "print(origami_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's shuffle this list randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(origami_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll turn this into two `numpy` arrays to train our model. The class names are easy enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crane' 'butterfly' 'frog' 'duck' 'star']\n",
      "['crane' 'butterfly' 'frog' 'crane' 'duck' 'butterfly' 'star' 'butterfly'\n",
      " 'duck' 'butterfly' 'duck' 'butterfly' 'star' 'duck' 'star' 'butterfly'\n",
      " 'duck' 'frog' 'frog' 'star' 'duck' 'duck' 'duck' 'butterfly' 'frog'\n",
      " 'frog' 'butterfly' 'frog' 'star' 'star' 'frog' 'duck' 'frog' 'crane'\n",
      " 'duck' 'butterfly' 'crane' 'duck' 'butterfly' 'butterfly' 'frog'\n",
      " 'butterfly' 'crane' 'star' 'frog' 'frog' 'crane' 'butterfly' 'duck'\n",
      " 'butterfly' 'frog' 'frog' 'duck' 'crane' 'duck' 'butterfly' 'duck'\n",
      " 'butterfly' 'butterfly' 'frog' 'star' 'butterfly' 'crane' 'duck' 'star'\n",
      " 'crane' 'star' 'duck' 'crane' 'frog' 'star' 'star' 'duck' 'butterfly'\n",
      " 'frog' 'star' 'frog' 'frog' 'crane' 'crane' 'crane' 'crane' 'frog'\n",
      " 'butterfly' 'crane' 'butterfly' 'frog' 'crane' 'butterfly' 'frog' 'star'\n",
      " 'duck' 'crane' 'frog' 'duck' 'star' 'star' 'crane' 'butterfly' 'crane'\n",
      " 'star' 'frog' 'frog' 'duck' 'star' 'frog' 'star' 'duck' 'duck' 'star'\n",
      " 'star' 'star' 'crane' 'star' 'crane' 'frog' 'butterfly' 'duck' 'frog'\n",
      " 'butterfly' 'duck' 'duck' 'frog' 'frog' 'crane' 'butterfly' 'crane'\n",
      " 'butterfly' 'star' 'butterfly' 'star' 'butterfly' 'frog' 'star' 'duck'\n",
      " 'frog' 'butterfly' 'duck' 'duck' 'crane' 'star' 'star' 'duck' 'duck'\n",
      " 'duck' 'crane' 'duck' 'star' 'butterfly' 'frog' 'crane' 'crane'\n",
      " 'butterfly' 'star' 'butterfly' 'crane' 'crane' 'butterfly' 'star'\n",
      " 'butterfly' 'star' 'crane' 'frog' 'crane' 'duck' 'star']\n"
     ]
    }
   ],
   "source": [
    "image_classes = []\n",
    "class_names = []\n",
    "for img in origami_images:\n",
    "    image_classes.append(img[0])\n",
    "    if img[0] not in class_names:\n",
    "        class_names.append(img[0])\n",
    "image_classes = np.array(image_classes)\n",
    "class_names = np.array(class_names)\n",
    "print(class_names)\n",
    "print(image_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166, 5)\n"
     ]
    }
   ],
   "source": [
    "def get_label(label):\n",
    "    return label == class_names\n",
    "\n",
    "origami_y = []\n",
    "for label in image_classes:\n",
    "    origami_y.append(get_label(label))\n",
    "origami_y = tf.convert_to_tensor(origami_y)\n",
    "print(origami_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are a little harder. Let's define a couple of functions to help with image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side length of normalized image\n",
    "IMG_SIZE = 256\n",
    "\n",
    "# Function to decode an image, render in grayscale and square dimensions\n",
    "def decode_img(img):\n",
    "    img = tf.io.decode_image(img, expand_animations=False)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float64)\n",
    "    img = tf.image.rgb_to_grayscale(img)\n",
    "    return tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "\n",
    "# Function to process a file path and return the image\n",
    "def process_path(img_path):\n",
    "    # Decode image using auxiliary function\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = decode_img(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so now let's create our second array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 files processed.\n",
      "20 files processed.\n",
      "30 files processed.\n",
      "40 files processed.\n",
      "50 files processed.\n",
      "60 files processed.\n",
      "70 files processed.\n",
      "80 files processed.\n",
      "90 files processed.\n",
      "100 files processed.\n",
      "110 files processed.\n",
      "120 files processed.\n",
      "130 files processed.\n",
      "140 files processed.\n",
      "150 files processed.\n",
      "160 files processed.\n",
      "All files processed. Converting to tensor.\n",
      "(166, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "origami_X = []\n",
    "counter = 0\n",
    "for img in origami_images:\n",
    "    #print(img[1])\n",
    "    origami_X.append(process_path(img[1]))\n",
    "    counter += 1\n",
    "    if counter % 10 == 0:\n",
    "        print('{} files processed.'.format(counter))\n",
    "print('All files processed. Converting to tensor.')\n",
    "origami_X = tf.convert_to_tensor(origami_X)\n",
    "print(origami_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's give this CNN thing a whirl..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = keras.Sequential()\n",
    "# Layer construction w/ L2 regularizers on convolutional layers\n",
    "cnn.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
    "cnn.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "cnn.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn.add(keras.layers.Flatten())\n",
    "cnn.add(keras.layers.Dense(16, activation='relu'))\n",
    "cnn.add(keras.layers.Dense(len(class_names), activation='softmax'))\n",
    "cnn.add(keras.layers.Reshape((-1,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 254, 254, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 127, 127, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 125, 125, 64)      18496     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1000000)           0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                16000016  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 85        \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, None)              0         \n",
      "=================================================================\n",
      "Total params: 16,018,917\n",
      "Trainable params: 16,018,917\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.compile(optimizer='adam',\n",
    "           loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "           metrics=[keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                    'categorical_accuracy'])\n",
    "cnn.summary()\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 166 samples\n",
      "Epoch 1/16\n",
      "166/166 [==============================] - 37s 222ms/sample - loss: 1.7031 - categorical_crossentropy: 1.6853 - categorical_accuracy: 0.1807\n",
      "Epoch 2/16\n",
      "166/166 [==============================] - 20s 121ms/sample - loss: 1.7060 - categorical_crossentropy: 1.6878 - categorical_accuracy: 0.1988\n",
      "Epoch 3/16\n",
      "166/166 [==============================] - 19s 117ms/sample - loss: 1.7060 - categorical_crossentropy: 1.7330 - categorical_accuracy: 0.1988\n",
      "Epoch 4/16\n",
      "166/166 [==============================] - 18s 109ms/sample - loss: 1.7060 - categorical_crossentropy: 1.6878 - categorical_accuracy: 0.1988\n",
      "Epoch 5/16\n",
      "166/166 [==============================] - 19s 117ms/sample - loss: 1.7060 - categorical_crossentropy: 1.6652 - categorical_accuracy: 0.1988\n",
      "Epoch 6/16\n",
      "166/166 [==============================] - 18s 110ms/sample - loss: 1.7060 - categorical_crossentropy: 1.7104 - categorical_accuracy: 0.1988\n",
      "Epoch 7/16\n",
      "166/166 [==============================] - 17s 104ms/sample - loss: 1.7060 - categorical_crossentropy: 1.6878 - categorical_accuracy: 0.1988\n",
      "Epoch 8/16\n",
      "166/166 [==============================] - 22s 134ms/sample - loss: 1.7060 - categorical_crossentropy: 1.7330 - categorical_accuracy: 0.1988\n",
      "Epoch 9/16\n",
      "166/166 [==============================] - 22s 131ms/sample - loss: 1.7060 - categorical_crossentropy: 1.6878 - categorical_accuracy: 0.1988\n",
      "Epoch 10/16\n",
      "166/166 [==============================] - 18s 111ms/sample - loss: 1.7060 - categorical_crossentropy: 1.7104 - categorical_accuracy: 0.1988\n",
      "Epoch 11/16\n",
      "166/166 [==============================] - 18s 109ms/sample - loss: 1.7060 - categorical_crossentropy: 1.6878 - categorical_accuracy: 0.1988\n",
      "Epoch 12/16\n",
      "166/166 [==============================] - 17s 104ms/sample - loss: 1.7060 - categorical_crossentropy: 1.7104 - categorical_accuracy: 0.1988\n",
      "Epoch 13/16\n",
      "166/166 [==============================] - 18s 107ms/sample - loss: 1.7060 - categorical_crossentropy: 1.7330 - categorical_accuracy: 0.1988\n",
      "Epoch 14/16\n",
      "166/166 [==============================] - 17s 101ms/sample - loss: 1.7060 - categorical_crossentropy: 1.7330 - categorical_accuracy: 0.1988\n",
      "Epoch 15/16\n",
      "166/166 [==============================] - 21s 128ms/sample - loss: 1.7060 - categorical_crossentropy: 1.7330 - categorical_accuracy: 0.1988\n",
      "Epoch 16/16\n",
      "166/166 [==============================] - 19s 113ms/sample - loss: 1.7060 - categorical_crossentropy: 1.6878 - categorical_accuracy: 0.1988\n"
     ]
    }
   ],
   "source": [
    "history = cnn.fit(origami_X, origami_y, epochs=16, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2966f791da89977f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2966f791da89977f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
